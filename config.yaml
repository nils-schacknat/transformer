transformer_params:
  stack_size: 6               # Encoder / decoder stack size
  model_dim: 256              # Output dim of each model (sub) layer
  ff_hidden_layer_dim: 2048   # FFN hidden layer size
  num_attention_heads: 8      # Number of attention heads
  key_dim: 32                 # Key dim
  value_dim: 32               # Value dim
  p_dropout: .1               # Dropout probability

datapipe_train:
  source_file: "translation_task/europarl-v7.de-en_train.de"
  target_file: "translation_task/europarl-v7.de-en_train.en"
  max_token_count: 128        # Approximate count of src_tokens + tgt_tokens + padding, 50000 in paper
  buffer_size: 6400           # Buffer size for batching

datapipe_val:
  source_file: "translation_task/europarl-v7.de-en_val.de"
  target_file: "translation_task/europarl-v7.de-en_val.en"
  max_token_count: 128        # Approximate count of src_tokens + padding
  buffer_size: 6400           # Buffer size for batching

training:
  label_smoothing: .1
  warmup_steps: 4000
  log_dir: "logs"

tokenizer:
  source_tokenizer_path: "tokenizer/tokenizer_de_lowercase_15000.model"
  target_tokenizer_path: "tokenizer/tokenizer_en_lowercase_5000.model"

max_generation_length: 64

num_steps:
  num_training_steps: 100
  num_validation_steps: 2
  num_validation_runs: 10
