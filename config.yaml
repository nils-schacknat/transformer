transformer_params:
  stack_size: 6
  model_dim: 512
  ff_hidden_layer_dim: 2048
  num_attention_heads: 8
  key_dim: 64
  value_dim: 64
  p_dropout: .1

training:
  label_smoothing: .1
  warmup_steps: 4000
  log_dir: "logs"
  max_generation_length: 256

tokenizer:
  tokenizer_path: "tokenizer/shared_vocab_tokenizer.model"

datapipe_params:
  max_token_count: 37500  # This includes padding tokens
  buffer_size: 96000      # Buffer size for bucketing

num_steps:
  num_training_steps: 100000
  num_testing_runs: 10

logging_level: "INFO"
