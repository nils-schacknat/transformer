transformer_params:
  stack_size: 6
  model_dim: 512
  ff_hidden_layer_dim: 2048
  num_attention_heads: 8
  key_dim: 64
  value_dim: 64
  p_dropout: .1

training:
  label_smoothing: .1
  warmup_steps: 4000
  log_dir: "logs"

tokenizer:
  tokenizer_path: "tokenizer/shared_vocab_tokenizer.model"

max_token_count: 25000
max_generation_length: 256

num_steps:
  num_training_steps: 100000
  num_testing_runs: 10
